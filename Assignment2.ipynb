{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "from conll import evaluate\n",
    "from conll import read_corpus_conll\n",
    "import pandas as pd\n",
    "\n",
    "corpus = read_corpus_conll('test.txt')\n",
    "sentence = \"Apple's Steve Jobs died in 2011 in Palo Alto, California.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Evaluate spaCy NER on CoNLL 2003 data, token-level performance (per class and total)\n",
    "- accuracy of correctly recognizing all tokens that belong to named entities\n",
    "\n",
    "First thing is to correctly read the sentences inside the CoNLL dataset, which is done by using the function `read_corpus_conll` from the conll.py script. But since its output are the sentences collected in a `list of list of list`, I implemented a function (`getSentences`) which takes as input the result of `read_corpus_conll`, and will output `list` where the full sentences from the dataset are stored. After collecting all of the sentences that will be then fed to `spacy`, using the function `getSentencesTag` I can create a `list of list of tuple` where the ground truth for the evaluation is stored. This was done basically in the same way as for the `getSentences` function, but istead of simply appending each token to the other, I create a tuple of `token` and `ent_tag` which will be used in the comparison of spacy's NER. \n",
    "Speaking of which, with the function `getSpacyTag` I take as input the `list` of sentences, and for each one of them will create a `list of tuples` that will contain `token` and `ent_tag`, but since `spacy` has a different way of tokenizing certain words some manipulation was done (for example dates like `19-04-2020` would be splitted in three for each number, while on the CoNLL dataset it is trated as a single token, so with the use of the `whitespace` propriety I was able to detecte those istances and with the help of a function I created `getWord`, which takes a `doc` object and the index numeber of the processed token and will output the full \"span\" that constitute as the whole token in the CoNLL dataset and another index number of the immidiate next token that will come after the processed one (index that will be used in the `getSpacyTag` function to essentially skip those \"extra\" tokens that are recognized by `spacy` but are already given out by the `getWord` function.\n",
    "\n",
    "So now that I all have all that's necessary to make an evaluation I create two variables to count all of the tokens and the ones that are correctly recognized, I also create two `dict` to store all the counts and the correct ones that will have as keys the four types of entities recognized in the CoNLL dataset times two because the tokens can be inside the entity `I-ENT` or its beginning `B-ENT`, and also a key for the non-entity type `O`. \n",
    "\n",
    "I should also point out that since CoNLL \"only\" uses four entity types while spacy recognizes 18 of them, a part of the `getSpacyTag` is dedicated to correctly associating `spacy` tags to the CoNLL ones, basically if a spacy tag was `GPE` it was reported as `LOC`, and the `PERSON` one was simply converted to `PER`, the `LOC` and `ORG` remained untouched while all of the others were assigned to the `MISC` label. \n",
    "\n",
    "Another thing I fell the need to point out is the reason why I computed the accuracy \"by hand\" is because the request was to compute the token-level preformance total and per class, and to do the latter I found out that using the function the professor suggested on piazza it was not possible. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Accuracy: 0.8645200010437596\n",
      "PER Accuracy: 0.6891453299675442\n",
      "ORG Accuracy: 0.37219551282051283\n",
      "LOC Accuracy: 0.6545454545454545\n",
      "MISC Accuracy: 0.5185185185185185\n",
      "Total Accuracy: 0.812038333153871\n"
     ]
    }
   ],
   "source": [
    "def getSentences(corpus):\n",
    "    sentence=''\n",
    "    sentences=[]\n",
    "    space= ' '\n",
    "    for s in corpus:\n",
    "        for i in s:\n",
    "            #print(i[0])\n",
    "            words=i[0].split()\n",
    "            word=words[0]\n",
    "            if(word=='-DOCSTART-'):\n",
    "                continue\n",
    "            else:\n",
    "                sentence=sentence+space+word\n",
    "        sent = sentence.strip()\n",
    "        if (len(sent)>0):\n",
    "            sentences.append(sent)\n",
    "        sentence=''\n",
    "    return sentences   \n",
    "        \n",
    "def getSentencesTag(corpus):\n",
    "    sentences=[]\n",
    "    for s in corpus:\n",
    "        sentence=[]\n",
    "        for i in s:\n",
    "            words=i[0].split()\n",
    "            tup=[words[0], words[3]]\n",
    "            if(words[0]=='-DOCSTART-'):\n",
    "                continue\n",
    "            #elif(words[0]=='.'):\n",
    "                #sentence.append(tup)\n",
    "            else:\n",
    "                sentence.append(tup)\n",
    "        if (len(sentence)>0):\n",
    "            sentences.append(sentence)\n",
    "    return sentences\n",
    "\n",
    "def getWord(doc, i):\n",
    "    token=doc[i]\n",
    "    word=token.text\n",
    "    nex=doc[i+1]\n",
    "    next_word=nex.text\n",
    "    #if (next_word == ',' ):\n",
    "    #    return word, i\n",
    "    for skip in range(i+1,len(doc)):\n",
    "        token=doc[skip]\n",
    "        if(token.whitespace_==''):\n",
    "            word+=doc[skip].text\n",
    "        else:\n",
    "            word+=doc[skip].text\n",
    "            break\n",
    "    skip+=1\n",
    "    return word, skip\n",
    "\n",
    "def getSpacyTag(sentences):\n",
    "    spacyTag=[]\n",
    "    for i,sent in enumerate(sentences):\n",
    "        doc=nlp(sent)\n",
    "        sentTag=[]\n",
    "        numberSkip=0\n",
    "        for j, t in enumerate(doc):\n",
    "            if(j>=numberSkip):\n",
    "                skip=False\n",
    "            if(t.whitespace_=='' and not(len(doc)==j+1)and not(skip)):\n",
    "                text, numberSkip=getWord(doc, j)\n",
    "                skip=True\n",
    "                sentTag.append([text, tag])\n",
    "            if (t.ent_iob_=='O' and not(skip)): \n",
    "                tag=t.ent_iob_ \n",
    "                text=t.text\n",
    "            elif(not(skip)):\n",
    "                if(t.ent_type_=='GPE'):\n",
    "                    tag=t.ent_iob_+'-LOC'\n",
    "                elif(t.ent_type_=='PERSON'):\n",
    "                    tag=t.ent_iob_+'-PER'\n",
    "                elif(t.ent_type_=='LOC' or t.ent_type_=='ORG'):\n",
    "                    tag=t.ent_iob_+'-'+t.ent_type_\n",
    "                else:       \n",
    "                    tag=t.ent_iob_+'-MISC'\n",
    "                text=t.text\n",
    "                \n",
    "            if(not(skip)):\n",
    "                sentTag.append([text, tag])\n",
    "        spacyTag.append(sentTag)\n",
    "    return spacyTag   \n",
    "\n",
    "sentences = getSentences(corpus)\n",
    "sentencesTag= getSentencesTag(corpus)\n",
    "spacyTag=getSpacyTag(sentences)\n",
    "        \n",
    "count=0\n",
    "correct=0\n",
    "correctDict={\"O\":0, \"B-PER\":0, \"B-LOC\":0, \"B-ORG\":0, \"I-PER\":0, \"I-LOC\":0, \"I-ORG\":0, \"B-MISC\":0, \"I-MISC\":0}\n",
    "countDict={\"O\":0, \"B-PER\":0, \"B-LOC\":0, \"B-ORG\":0, \"I-PER\":0, \"I-LOC\":0, \"I-ORG\":0, \"B-MISC\":0, \"I-MISC\":0}\n",
    "for i,sent in enumerate(spacyTag):\n",
    "    gt=sentencesTag[i]\n",
    "    for j,token in enumerate(sent):\n",
    "        count+=1\n",
    "        countDict[gt[j][1]]+=1\n",
    "        if(token==gt[j]):\n",
    "            correct+=1\n",
    "            correctDict[token[1]]+=1\n",
    "\n",
    "print('O Accuracy:', correctDict.get(\"O\")/countDict.get(\"O\"))\n",
    "print('PER Accuracy:', (correctDict.get(\"B-PER\")+correctDict.get(\"I-PER\"))/(countDict.get(\"B-PER\")+countDict.get(\"I-PER\")))\n",
    "print('ORG Accuracy:', (correctDict.get(\"B-ORG\")+correctDict.get(\"I-ORG\"))/(countDict.get(\"B-ORG\")+countDict.get(\"I-ORG\")))\n",
    "print('LOC Accuracy:', (correctDict.get(\"B-LOC\")+correctDict.get(\"I-LOC\"))/(countDict.get(\"B-LOC\")+countDict.get(\"I-LOC\")))\n",
    "print('MISC Accuracy:', (correctDict.get(\"B-MISC\")+correctDict.get(\"I-MISC\"))/(countDict.get(\"B-MISC\")+countDict.get(\"I-MISC\")))\n",
    "print ('Total Accuracy:', correct/count)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Evaluate spaCy NER on CoNLL 2003 data, chunk-level performance (per class and total)\n",
    "- precision, recall, f-measure of correctly recognizing all the named entities in a chunk per class and total \n",
    "\n",
    "Since the previously generated `list of list of tuple` was already the correct input for the function `evaluate` inside the `conll.py` script, I simply generated the results and printed them using the `panda` library "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>p</th>\n",
       "      <th>r</th>\n",
       "      <th>f</th>\n",
       "      <th>s</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PER</th>\n",
       "      <td>0.747</td>\n",
       "      <td>0.594</td>\n",
       "      <td>0.662</td>\n",
       "      <td>1617</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ORG</th>\n",
       "      <td>0.450</td>\n",
       "      <td>0.267</td>\n",
       "      <td>0.335</td>\n",
       "      <td>1661</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC</th>\n",
       "      <td>0.727</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.693</td>\n",
       "      <td>1668</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MISC</th>\n",
       "      <td>0.107</td>\n",
       "      <td>0.540</td>\n",
       "      <td>0.178</td>\n",
       "      <td>702</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>0.393</td>\n",
       "      <td>0.511</td>\n",
       "      <td>0.445</td>\n",
       "      <td>5648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           p      r      f     s\n",
       "PER    0.747  0.594  0.662  1617\n",
       "ORG    0.450  0.267  0.335  1661\n",
       "LOC    0.727  0.661  0.693  1668\n",
       "MISC   0.107  0.540  0.178   702\n",
       "total  0.393  0.511  0.445  5648"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results=evaluate(sentencesTag,spacyTag)\n",
    "pd_tbl = pd.DataFrame().from_dict(results, orient='index')\n",
    "pd_tbl.round(decimals=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Grouping of Entities. Write a function to group recognized named entities using `noun_chunks` method of spaCy. \n",
    "\n",
    "The function `groupEnteties` takes as input a sentence and will output a list of all the entities recognized by `spacy` but grouped by noun chunks. Initially I create lists for the entites and the noun chunks, then I cycle throug all of the entities and using a function `findChunk` I check if an entity is inside one the the chunks, if it is then it will return a `bool` value `True` and the text of the chunk it is part of, if it is not part of any chunk the output will be `False` and an empty string. So by making use of a `temp` list, I monitor if the entity I'm on is still part of the same chunk as the one of the previous entity, if I am I simply append the tag of the entity to the one of the previous one, if I'm still in a chunk then probably I'm in a new chunk, so I can add to the output list `temp` if it's not empty, and then work on this new chunk. If I'm not in a chunk then again I can add to the output list `temp` if it's not empty, and thenadd to the output list this entity outside every chunk that still needs to be added since it is an entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['ORG', 'PERSON'], ['DATE'], ['GPE'], ['GPE']]\n"
     ]
    }
   ],
   "source": [
    "def findChunk(substring,chunks):\n",
    "    for chunk in chunks:\n",
    "        if substring in chunk.text:\n",
    "            return True, chunk.text\n",
    "    return False, ''\n",
    "\n",
    "def groupEnteties(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    labels=[] #list of all the entities\n",
    "    for i in doc.ents:\n",
    "        labels.append([i.text,i.label_])\n",
    "    chunks=[]\n",
    "    for i in doc.noun_chunks:\n",
    "        chunks.append(i)\n",
    "    out=[]\n",
    "    temp = []\n",
    "    prev_chunk = ''\n",
    "    for i, ents in enumerate(labels):\n",
    "        ret, chunk = findChunk(ents[0], chunks) \n",
    "        if i==0:\n",
    "            prev_chunk = chunk\n",
    "        if (ret and chunk == prev_chunk):\n",
    "            temp.append(ents[1])\n",
    "        elif(ret):\n",
    "            #un chunk dietro l'altro ma diverso da quello di prima\n",
    "            if(len(temp)>0):\n",
    "                out.append(temp) #quindi aggiung subito i tag del chunk precedente\n",
    "            prev_chunk=chunk\n",
    "            temp=[]\n",
    "            temp.append(ents[1])\n",
    "        else:\n",
    "            if(len(temp)>0):\n",
    "                out.append(temp)\n",
    "                temp=[]\n",
    "            out.append([ents[1]])\n",
    "    if(len(temp)>0):\n",
    "        out.append(temp)\n",
    "    return out\n",
    "\n",
    "print(groupEnteties(sentence))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Analyze the groups in terms of most frequent combinations\n",
    "\n",
    "With the function `getFrequencies` that takes as input a list of sentences and as output a `dict` that as keys has the combinations of possible entities. For each sentence the function `groupEnteties` is called to compute the entities grouped by noun chunks, I then iterate over it and if it has more than one element (basically at least two entities grouped together) then a count in the dictionary will be added. To do so I check if the dictionary alredy has a key which is the cobination of those entities, if it does I add one to the count, if it's the first time it has come up (so it doesn't exist yet a key with that name) I create this new entry key and initiaze it to one.\n",
    "Since the request was to report the most frequent combination I use the function `nbest` to output the top 5 combiations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CARDINAL PERSON = 49\n",
      "NORP PERSON = 42\n",
      "GPE PERSON = 33\n",
      "GPE GPE = 28\n",
      "ORG PERSON = 20\n"
     ]
    }
   ],
   "source": [
    "def getFrequencies(sentences):\n",
    "    #x = []\n",
    "    d = dict()\n",
    "    for sent in sentences:\n",
    "        temp = groupEnteties(sent)\n",
    "        for t in temp:\n",
    "            if (len(t)>1):\n",
    "                key=t[0]\n",
    "                space = ' '\n",
    "                for i, lab in enumerate(t):\n",
    "                    if(i>0):\n",
    "                        key = key + space + t[i]\n",
    "                if (key in d.keys()):\n",
    "                    d[key]+=1\n",
    "                else:\n",
    "                    d[key]=1\n",
    "    return d\n",
    "\n",
    "def nbest(d, n=1):\n",
    "    return dict(sorted(d.items(), key=lambda item: item[1], reverse=True)[:n])\n",
    "\n",
    "sentences = getSentences(corpus)\n",
    "diz = getFrequencies(sentences)\n",
    "\n",
    "d = nbest(diz, 5)\n",
    "for c in d:\n",
    "    print(c, '=', d.get(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##One of the possible post-processing steps is to fix segmentation errors. Write a function that extends the entity span to cover the full noun-compounds. Make use of `compound` dependency relation.\n",
    "\n",
    "For this task I created a a function `fixEntity` that takes as input a sentence and returns `list of tuples` where the first element is made by the `token.text` and the second is the entity iob tag and type where if a token has the dependecy `conpound` and itself is not part of eny entity while it's head is then the tag of this `conpound` token is changes to be considered as part of the entity, and the head too has the iob tag changed if the `conpound` token actually comes before it (so the tag is changed to `I` because its inside the entity and possibly not in the beginning). To do so a `list` change is used so that before the function gives the output it updates all of the changed heads for the conpound tokens. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Apple', 'B-ORG']\n",
      "[\"'s\", 'O']\n",
      "['Steve', 'B-PERSON']\n",
      "['Jobs', 'I-PERSON']\n",
      "['died', 'O']\n",
      "['in', 'O']\n",
      "['2011', 'B-DATE']\n",
      "['in', 'O']\n",
      "['Palo', 'B-GPE']\n",
      "['Alto', 'I-GPE']\n",
      "[',', 'O']\n",
      "['California', 'B-GPE']\n",
      "['.', 'O']\n"
     ]
    }
   ],
   "source": [
    "def fixEntity(sentence):\n",
    "    doc = nlp(sentence)\n",
    "    ret =[]\n",
    "    change = []\n",
    "    for i, token in enumerate(doc):\n",
    "        if (token.dep_ == 'compound' and token.ent_type_ == '' and token.head.ent_type_ != ''):\n",
    "            iToken=token.i\n",
    "            iHead=token.head.i\n",
    "            #check = (iToken-iHead)*(iToken-iHead)\n",
    "            if(iToken-iHead<0): \n",
    "                tag = 'B-'+token.head.ent_type_\n",
    "                change.append([token.head.i-1, 'I-'+token.head.ent_type_])\n",
    "                temp = [token.text, tag]\n",
    "                ret.append(temp)\n",
    "                continue\n",
    "            elif (token.head.ent_iob_ == 'I'):\n",
    "                tag = 'B-'+token.head.ent_type_\n",
    "                temp = [token.text, tag]\n",
    "                ret.append(temp)\n",
    "                continue\n",
    "        if token.ent_type_ == '':\n",
    "            tag = token.ent_iob_\n",
    "        else:\n",
    "            tag = token.ent_iob_+'-'+token.ent_type_\n",
    "        temp = [token.text, tag]\n",
    "        ret.append(temp)\n",
    "    if (len(change)>0):\n",
    "        for c in change:\n",
    "            ret[c[0]]=[ret[c[0]][0],c[1]]\n",
    "    return ret\n",
    "\n",
    "test = fixEntity(sentence)\n",
    "for i in test:\n",
    "    print(i)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
